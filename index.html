<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Xibin Song</title>

    <meta name="author" content="Xibin Song">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Xibin Song
                </p>
                <p>I'm a research scientist at Tencent XR vision Labs.
                </p>
                <p>
                  Before that, I work as senior researcher at Robotics and Autonomous Driving Laboratory (RAL) of Baidu Research in 2018-2022. I did my PhD and Bachelor at Shandong University, where I was advised by <a href="https://irc.cs.sdu.edu.cn/info/1030/1371.htm"> Prof. Xueying Qin</a>. During my PhD, I worked as a joint training of Ph.D. student in the Research School of Engineering at the Australian National University in 2015-2016, where I was advised by <a href="https://scholar.google.com/citations?user=cHia5p0AAAAJ&hl=en"> Prof. Richard Hartley </a> and <a href="https://sites.google.com/site/daiyuchao/"> Prof. Yuchao Dai</a>. 
                </p>
                <p style="text-align:center">
                  <a href="mailto:song.sducg@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?view_op=list_works&hl=zh-CN&hl=zh-CN&user=2gudyEQAAAAJ&sortby=pubdate">Scholar</a> &nbsp;/&nbsp;
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/xibinsong.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/xibinsong.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research interests lie in the intersection of 3D computer vision, artificial intelligence, particularly focusing on Perception Technology in Autonomous Driving, 3D AIGC and AR/VR, etc.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<tr onmouseout="r2r_stop()" onmouseover="r2r_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/lam3d-pipeline.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function r2r_start() {
            document.getElementById('r2r_image').style.opacity = "1";
          }

          function r2r_stop() {
            document.getElementById('r2r_image').style.opacity = "0";
          }
          r2r_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://relight-to-reconstruct.github.io/">
          <span class="papertitle">LAM3D: Large Image-Point-Cloud Alignment Model for 3D Reconstruction from Single Image</span>
        </a>
        <br>
        <a>Ruikai Cui</a>,
        <strong>Xibin Song</strong>,
	<a>Weixuan Sun</a>, 
        <a>Senbo Wang</a>,
        <a>Weizhe Liu</a>, 
        <a>Shenzhou Chen</a>,
	<a>Taizhang Shang</a>,
	<a>Yang Li</a>,
	<a>Nick Barnes</a>,
	<a>Hongdong Li</a>,
	<a>Pan Ji</a>
	<br>
        <em>Corresponding author</em>
        <br>
        <a href="https://arxiv.org/pdf/2405.15622">NIPS2024</a>
        <p></p>
        <p>
				Large Image and Point Cloud Alignment Model (LAM3D), which utilizes 3D point cloud data to enhance the fidelity of generated 3D meshes.
        </p>
      </td>
    </tr>

    <tr onmouseout="r2r_stop()" onmouseover="r2r_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/NeuSDFusion.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function r2r_start() {
            document.getElementById('r2r_image').style.opacity = "1";
          }

          function r2r_stop() {
            document.getElementById('r2r_image').style.opacity = "0";
          }
          r2r_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://relight-to-reconstruct.github.io/">
          <span class="papertitle">Neusdfusion: A spatial-aware generative model for 3d shape completion, reconstruction, and generation</span>
        </a>
        <br>
        <a>Ruikai Cui</a>,
        <a>Weizhe Liu</a>,
	<a>Weixuan Sun</a>, 
        <a>Senbo Wang</a>,
        <a>Taizhang Shang</a>, 
        <a>Yang Li</a>,
	<strong>Xibin Song</strong>,
	<a>Han Yan</a>,
	<a>Zhennan Wu</a>,
	<a>Shenzhou Chen</a>,
	<a>Hongdong Li</a>,
	<a>Pan Ji</a>
        <a href="https://arxiv.org/pdf/2403.18241">ECCV2024</a>
        <p></p>
        <p>
		3D shape generation aims to produce innovative 3D content adhering to specific conditions and constraints. Existing methods often decompose 3D shapes into a sequence of localized components, treating each element in isolation without considering spatial consistency. As a result, these approaches exhibit limited versatility in 3D data representation and shape generation, hindering their ability to generate highly diverse 3D shapes that comply with the specified constraints. In this paper, we introduce a novel spatial-aware 3D shape generation framework that leverages 2D plane representations for enhanced 3D shape modeling.
        </p>
      </td>
    </tr>


    <tr onmouseout="simvs_stop()" onmouseover="simvs_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/SRNSD.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function simvs_start() {
            document.getElementById('simvs_image').style.opacity = "1";
          }

          function simvs_stop() {
            document.getElementById('simvs_image').style.opacity = "0";
          }
          simvs_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://nerf-casting.github.io/">
          <span class="papertitle">SRNSD: Structure-Regularized Night-Time Self-Supervised Monocular Depth Estimation for Outdoor Scenes</span>
        </a>
        <br>
        <a>Ruimin Cong</a>,
        <a>Chunlei Wu</a>,
        <strong>Xibin Song</strong>,
        <a>Wei Zhang</a>,
        <a>Sam Kwong</a>,
        <a>Hongdong Li</a>,
        <a>Pan Ji</a>
	<br>
        <em>Corresponding author</em>
        <br>
        <a href="https://ieeexplore.ieee.org/abstract/document/10696933">IEEE Transactions on Image Processing (2024)</a>
        <p></p>
        <p>
        Night-time self-supervised Depth Estimation for Outdoor Scenes.
        </p>
      </td>
    </tr>

    <tr onmouseout="agdf_stop()" onmouseover="agdf_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/domain-depth.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function agdf_start() {
            document.getElementById('agdf_image').style.opacity = "1";
          }

          function agdf_stop() {
            document.getElementById('agdf_image').style.opacity = "0";
          }
          agdf_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://nerf-casting.github.io/">
          <span class="papertitle">AGDF-Net: learning domain generalizable depth features with adaptive guidance fusion</span>
        </a>
        <br>
				<a>Lina Liu</a>,
				<strong>Xibin Song</strong>,
				<a>Mengmeng Wang</a>,
				<a>Yuchao Dai</a>,
				<a>Yong Liu</a>,
	      			<a>Liangjun Zhang</a>
	<br>
        <em>Corresponding author</em>
        <br>
        <a href="https://ieeexplore.ieee.org/abstract/document/10696933">IEEE Transactions on Pattern Analysis and Machine Intelligence (2023)</a>
        <p></p>
	<p>
		We propose a domain generalizable feature extraction network with adaptive guidance fusion (AGDF-Net) to fully acquire essential features for depth estimation at multi-scale feature levels. Specifically, our AGDF-Net first separates the image into initial depth and weak-related depth components with reconstruction and contrary losses. Subsequently, an adaptive guidance fusion module is designed to sufficiently intensify the initial depth features for domain generalizable intensified depth features acquisition. Finally, taking intensified depth features as input, an arbitrary depth estimation network can be used for real-world depth estimation. Using only synthetic datasets, our AGDF-Net can be applied to various real-world datasets (i.e., KITTI, NYUDv2, NuScenes, DrivingStereo and CityScapes) with state-of-the-art performances. Furthermore, experiments with a small amount of real-world data in a semi-supervised setting also demonstrate the superiority of AGDF-Net over state-of-the-art approaches.
	</p>
      </td>
    </tr>

    <tr onmouseout="digging_stop()" onmouseover="digging_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/stereo-tpami-2023.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function digging_start() {
            document.getElementById('digging_image').style.opacity = "1";
          }

          function digging_stop() {
            document.getElementById('digging_image').style.opacity = "0";
          }
          simvs_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://nerf-casting.github.io/">
          <span class="papertitle">Digging into uncertainty-based pseudo-label for robust stereo matching</span>
        </a>
        <br>
				<a>Zhelun Shen</a>,
				<strong>Xibin Song</strong>,
				<a>Yuchao Dai</a>,
				<a>Dingfu Zhou</a>,
				<a>Zhibo Rao</a>,
	      			<a>Liangjun Zhang</a>
	<br>
        <em>Corresponding author</em>
        <br>
        <a href="https://arxiv.org/pdf/2307.16509">IEEE Transactions on Pattern Analysis and Machine Intelligence (2023)</a>
        <p></p>
	<p>
		We propose to dig into uncertainty estimation for robust stereo matching. Specifically, to balance the disparity distribution, we employ a pixel-level uncertainty estimation to adaptively adjust the next stage disparity searching space, in this way driving the network progressively prune out the space of unlikely correspondences. Then, to solve the limited ground truth data, an uncertainty-based pseudo-label is proposed to adapt the pre-trained model to the new domain, where pixel-level and area-level uncertainty estimation are proposed to filter out the high-uncertainty pixels of predicted disparity maps and generate sparse while reliable pseudo-labels to align the domain gap. Experimentally, our method shows strong cross-domain, adapt, and joint generalization and obtains \textbf{1st} place on the stereo task of Robust Vision Challenge 2020. Additionally, our uncertainty-based pseudo-labels can be extended to train monocular depth estimation networks in an unsupervised way and even achieves comparable performance with the supervised methods.
	</p>
      </td>
    </tr>

   <tr onmouseout="simvs_stop()" onmouseover="simvs_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/dehazing-tip-2023.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function simvs_start() {
            document.getElementById('simvs_image').style.opacity = "1";
          }

          function simvs_stop() {
            document.getElementById('simvs_image').style.opacity = "0";
          }
          simvs_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://nerf-casting.github.io/">
          <span class="papertitle">TUSR-Net: triple unfolding single image dehazing with self-regularization and dual feature to pixel attention</span>
        </a>
        <br>
        <strong>Xibin Song</strong>,
        <a>Dingfu Zhou</a>,
        <a>Wei Li</a>,
        <a>Yuchao Dai</a>,
        <a>Zhelun Shen</a>,
        <a>Liangjun Zhang</a>,
        <a>Hongdong Li</a>
	<p></p>
        <a href="https://ieeexplore.ieee.org/abstract/document/10043627">IEEE Transactions on Image Processing (2023)</a>
        <p></p>
	<p>
		We propose an end-to-end self-regularized network (TUSR-Net) which exploits the contrastive peculiarity of different components of the hazy image, \emph{i.e}, self-regularization (SR). In specific, the hazy image is separated into clear and hazy components and constraint between different image components, \emph{i.e.}, self-regularization, is leveraged to pull the recovered clear image closer to groundtruth, which largely promotes the performance of image dehazing. Meanwhile, an effective triple unfolding framework combined with dual feature to pixel attention is proposed to intensify and fuse the intermediate information in feature, channel and pixel levels, respectively, thus features with better representational ability can be obtained. Our TUSR-Net achieves better trade-off between performance and parameter size with weight-sharing strategy and is much more flexible. Experiments on various benchmarking datasets demonstrate the superiority of our TUSR-Net over state-of-the-art single image dehazing methods.
	</p>
      </td>
    </tr>

  </body>
</html>
