<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Xibin Song</title>

    <meta name="author" content="Xibin Song">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Xibin Song
                </p>
                <p>I'm a research scientist at Tencent XR vision Labs.
                </p>
                <p>
                  Before that, I work as senior researcher at Robotics and Autonomous Driving Laboratory (RAL) of Baidu Research in 2018-2022. I did my PhD and Bachelor at Shandong University, where I was advised by <a href="https://irc.cs.sdu.edu.cn/info/1030/1371.htm"> Prof. Xueying Qin</a>. During my PhD, I worked as a joint training of Ph.D. student in the Research School of Engineering at the Australian National University in 2015-2016, where I was advised by <a href="https://scholar.google.com/citations?user=cHia5p0AAAAJ&hl=en"> Prof. Richard Hartley </a> and <a href="https://sites.google.com/site/daiyuchao/"> Prof. Yuchao Dai</a>. 
                </p>
                <p style="text-align:center">
                  <a href="mailto:song.sducg@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?view_op=list_works&hl=zh-CN&hl=zh-CN&user=2gudyEQAAAAJ&sortby=pubdate">Scholar</a> &nbsp;/&nbsp;
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/xibinsong.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/xibinsong.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research interests lie in the intersection of 3D computer vision, artificial intelligence, particularly focusing on Perception Technology in Autonomous Driving, 3D AIGC and AR/VR, etc.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<tr onmouseout="r2r_stop()" onmouseover="r2r_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/lam3d-pipeline.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function r2r_start() {
            document.getElementById('r2r_image').style.opacity = "1";
          }

          function r2r_stop() {
            document.getElementById('r2r_image').style.opacity = "0";
          }
          r2r_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2405.15622">
          <span class="papertitle">LAM3D: Large Image-Point-Cloud Alignment Model for 3D Reconstruction from Single Image</span>
        </a>
        <br>
        <a>Ruikai Cui</a>,
        <strong>Xibin Song</strong>,
	<a>Weixuan Sun</a>, 
        <a>Senbo Wang</a>,
        <a>Weizhe Liu</a>, 
        <a>Shenzhou Chen</a>,
	<a>Taizhang Shang</a>,
	<a>Yang Li</a>,
	<a>Nick Barnes</a>,
	<a>Hongdong Li</a>,
	<a>Pan Ji</a>
	<br>
        <em>Corresponding author</em>
        <br>
        <a href="https://arxiv.org/pdf/2405.15622"> pdf - NIPS2024</a>
        <p></p>
        <p>
				Large Image and Point Cloud Alignment Model (LAM3D), which utilizes 3D point cloud data to enhance the fidelity of generated 3D meshes.
        </p>
      </td>
    </tr>

    <tr onmouseout="r2r_stop()" onmouseover="r2r_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/NeuSDFusion.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function r2r_start() {
            document.getElementById('r2r_image').style.opacity = "1";
          }

          function r2r_stop() {
            document.getElementById('r2r_image').style.opacity = "0";
          }
          r2r_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2403.18241">
          <span class="papertitle">Neusdfusion: A spatial-aware generative model for 3d shape completion, reconstruction, and generation</span>
        </a>
        <br>
        <a>Ruikai Cui</a>,
        <a>Weizhe Liu</a>,
	<a>Weixuan Sun</a>, 
        <a>Senbo Wang</a>,
        <a>Taizhang Shang</a>, 
        <a>Yang Li</a>,
	<strong>Xibin Song</strong>,
	<a>Han Yan</a>,
	<a>Zhennan Wu</a>,
	<a>Shenzhou Chen</a>,
	<a>Hongdong Li</a>,
	<a>Pan Ji</a>
	<p></p>
        <a href="https://arxiv.org/pdf/2403.18241"> pdf - ECCV2024</a>
        <p></p>
        <p>
		3D shape generation aims to produce innovative 3D content adhering to specific conditions and constraints. Existing methods often decompose 3D shapes into a sequence of localized components, treating each element in isolation without considering spatial consistency. As a result, these approaches exhibit limited versatility in 3D data representation and shape generation, hindering their ability to generate highly diverse 3D shapes that comply with the specified constraints. In this paper, we introduce a novel spatial-aware 3D shape generation framework that leverages 2D plane representations for enhanced 3D shape modeling.
        </p>
      </td>
    </tr>


    <tr onmouseout="simvs_stop()" onmouseover="simvs_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/SRNSD.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function simvs_start() {
            document.getElementById('simvs_image').style.opacity = "1";
          }

          function simvs_stop() {
            document.getElementById('simvs_image').style.opacity = "0";
          }
          simvs_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/10696933">
          <span class="papertitle">SRNSD: Structure-Regularized Night-Time Self-Supervised Monocular Depth Estimation for Outdoor Scenes</span>
        </a>
        <br>
        <a>Ruimin Cong</a>,
        <a>Chunlei Wu</a>,
        <strong>Xibin Song</strong>,
        <a>Wei Zhang</a>,
        <a>Sam Kwong</a>,
        <a>Hongdong Li</a>,
        <a>Pan Ji</a>
	<br>
        <em>Corresponding author</em>
        <br>
        <a href="https://ieeexplore.ieee.org/abstract/document/10696933"> pdf - IEEE Transactions on Image Processing (2024)</a>
        <p></p>
        <p>
        Night-time self-supervised Depth Estimation for Outdoor Scenes.
        </p>
      </td>
    </tr>

    <tr onmouseout="agdf_stop()" onmouseover="agdf_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/domain-depth.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function agdf_start() {
            document.getElementById('agdf_image').style.opacity = "1";
          }

          function agdf_stop() {
            document.getElementById('agdf_image').style.opacity = "0";
          }
          agdf_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/10696933">
          <span class="papertitle">AGDF-Net: learning domain generalizable depth features with adaptive guidance fusion</span>
        </a>
        <br>
				<a>Lina Liu</a>,
				<strong>Xibin Song</strong>,
				<a>Mengmeng Wang</a>,
				<a>Yuchao Dai</a>,
				<a>Yong Liu</a>,
	      			<a>Liangjun Zhang</a>
	<br>
        <em>Corresponding author</em>
        <br>
        <a href="https://ieeexplore.ieee.org/abstract/document/10696933"> pdf -IEEE Transactions on Pattern Analysis and Machine Intelligence (2023)</a>
        <p></p>
	<p>
		We propose a domain generalizable feature extraction network with adaptive guidance fusion (AGDF-Net) to fully acquire essential features for depth estimation at multi-scale feature levels. Specifically, our AGDF-Net first separates the image into initial depth and weak-related depth components with reconstruction and contrary losses. Subsequently, an adaptive guidance fusion module is designed to sufficiently intensify the initial depth features for domain generalizable intensified depth features acquisition. Finally, taking intensified depth features as input, an arbitrary depth estimation network can be used for real-world depth estimation. Using only synthetic datasets, our AGDF-Net can be applied to various real-world datasets (i.e., KITTI, NYUDv2, NuScenes, DrivingStereo and CityScapes) with state-of-the-art performances. Furthermore, experiments with a small amount of real-world data in a semi-supervised setting also demonstrate the superiority of AGDF-Net over state-of-the-art approaches.
	</p>
      </td>
    </tr>

    <tr onmouseout="digging_stop()" onmouseover="digging_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/stereo-tpami-2023.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function digging_start() {
            document.getElementById('digging_image').style.opacity = "1";
          }

          function digging_stop() {
            document.getElementById('digging_image').style.opacity = "0";
          }
          simvs_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2307.16509">
          <span class="papertitle">Digging into uncertainty-based pseudo-label for robust stereo matching</span>
        </a>
        <br>
				<a>Zhelun Shen</a>,
				<strong>Xibin Song</strong>,
				<a>Yuchao Dai</a>,
				<a>Dingfu Zhou</a>,
				<a>Zhibo Rao</a>,
	      			<a>Liangjun Zhang</a>
	<br>
        <em>Corresponding author</em>
        <br>
        <a href="https://arxiv.org/pdf/2307.16509"> pdf - IEEE Transactions on Pattern Analysis and Machine Intelligence (2023)</a>
        <p></p>
	<p>
		We propose to dig into uncertainty estimation for robust stereo matching. Specifically, to balance the disparity distribution, we employ a pixel-level uncertainty estimation to adaptively adjust the next stage disparity searching space, in this way driving the network progressively prune out the space of unlikely correspondences. Then, to solve the limited ground truth data, an uncertainty-based pseudo-label is proposed to adapt the pre-trained model to the new domain, where pixel-level and area-level uncertainty estimation are proposed to filter out the high-uncertainty pixels of predicted disparity maps and generate sparse while reliable pseudo-labels to align the domain gap. Experimentally, our method shows strong cross-domain, adapt, and joint generalization and obtains \textbf{1st} place on the stereo task of Robust Vision Challenge 2020. Additionally, our uncertainty-based pseudo-labels can be extended to train monocular depth estimation networks in an unsupervised way and even achieves comparable performance with the supervised methods.
	</p>
      </td>
    </tr>

   <tr onmouseout="simvs_stop()" onmouseover="simvs_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/dehazing-tip-2023.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function simvs_start() {
            document.getElementById('simvs_image').style.opacity = "1";
          }

          function simvs_stop() {
            document.getElementById('simvs_image').style.opacity = "0";
          }
          simvs_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/10043627">
          <span class="papertitle">TUSR-Net: triple unfolding single image dehazing with self-regularization and dual feature to pixel attention</span>
        </a>
        <br>
        <strong>Xibin Song</strong>,
        <a>Dingfu Zhou</a>,
        <a>Wei Li</a>,
        <a>Yuchao Dai</a>,
        <a>Zhelun Shen</a>,
        <a>Liangjun Zhang</a>,
        <a>Hongdong Li</a>
	<p></p>
        <a href="https://ieeexplore.ieee.org/abstract/document/10043627">pdf - IEEE Transactions on Image Processing (2023)</a>
        <p></p>
	<p>
		We propose an end-to-end self-regularized network (TUSR-Net) which exploits the contrastive peculiarity of different components of the hazy image, \emph{i.e}, self-regularization (SR). In specific, the hazy image is separated into clear and hazy components and constraint between different image components, \emph{i.e.}, self-regularization, is leveraged to pull the recovered clear image closer to groundtruth, which largely promotes the performance of image dehazing. Meanwhile, an effective triple unfolding framework combined with dual feature to pixel attention is proposed to intensify and fuse the intermediate information in feature, channel and pixel levels, respectively, thus features with better representational ability can be obtained. Our TUSR-Net achieves better trade-off between performance and parameter size with weight-sharing strategy and is much more flexible. Experiments on various benchmarking datasets demonstrate the superiority of our TUSR-Net over state-of-the-art single image dehazing methods.
	</p>
      </td>
    </tr>


   <tr onmouseout="simvs_stop()" onmouseover="simvs_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/mff-ral.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function simvs_start() {
            document.getElementById('simvs_image').style.opacity = "1";
          }

          function simvs_stop() {
            document.getElementById('simvs_image').style.opacity = "0";
          }
          simvs_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://april.zju.edu.cn/core/papercite-data/pdf/liu2023mff.pdf">
          <span class="papertitle">Mff-net: Towards efficient monocular depth completion with multi-modal feature fusion</span>
        </a>
        <br>
        <a>Lina Liu</a>,
        <strong>Xibin Song</strong>,
        <a>Jiadai Sun</a>,
        <a>Xiaoyang Lyu</a>,
        <a>Lin Li</a>,
        <a>Yong Liu</a>,
        <a>Liangjun Zhang</a>
	<br>
        <em>Corresponding author</em>
        <br>
	<p></p>
        <a href="https://april.zju.edu.cn/core/papercite-data/pdf/liu2023mff.pdf"> pdf - RAL (2023)</a>
        <p></p>
	<p>
		In this work, we propose an efficient multi-modal feature fusion based depth completion framework (MFF-Net), which can efficiently extract and fuse features with different modals in both encoding and decoding processes, thus more depth details with better performance can be obtained. 
	</p>
      </td>
    </tr>

   <tr onmouseout="simvs_stop()" onmouseover="simvs_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/stereo-eccv-2022.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function simvs_start() {
            document.getElementById('simvs_image').style.opacity = "1";
          }

          function simvs_stop() {
            document.getElementById('simvs_image').style.opacity = "0";
          }
          simvs_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2006.12797">
          <span class="papertitle">Pcw-net: Pyramid combination and warping cost volume for stereo matching</span>
        </a>
        <br>
        <a>Zhelun Shen</a>,
	<a>Yuchao Dai</a>,
        <strong>Xibin Song</strong>,
        <a>Zhibo Rao</a>,
        <a>Dingfu Zhou</a>,
        <a>Liangjun Zhang</a>
	<br>
        <em>Corresponding author</em>
        <br>
	<p></p>
        <a href="https://arxiv.org/pdf/2006.12797"> pdf - ECCV (2022)</a>
        <p></p>
	<p>
		 We propose PCW-Net, a Pyramid Combination and Warping cost volume-based network to achieve good performance on both crossdomain generalization and stereo matching accuracy on various benchmarks.
	</p>
      </td>
    </tr>

   <tr onmouseout="simvs_stop()" onmouseover="simvs_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/dehazing-tcsvt-2022.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function simvs_start() {
            document.getElementById('simvs_image').style.opacity = "1";
          }

          function simvs_stop() {
            document.getElementById('simvs_image').style.opacity = "0";
          }
          simvs_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/9893197">
          <span class="papertitle">WSAMF-Net: Wavelet spatial attention-based MultiStream feedback network for single image dehazing</span>
        </a>
        <br> 
        <strong>Xibin Song</strong>,
	<a>Dingfu Zhou</a>,
        <a>Wei Li</a>,
        <a>Haodong Ding</a>,
        <a>Yuchao Dai</a>,
        <a>Liangjun Zhang</a>
	<p></p>
        <a href="https://ieeexplore.ieee.org/abstract/document/9893197"> pdf - IEEE Transactions on Circuits and Systems for Video Technology (2022)</a>
        <p></p>
	<p>
	A wavelet spatial attention based multi-stream feedback network (WSAMF-Net) is proposed for effective single image dehazing. 	 
	</p>
      </td>
    </tr>

   <tr onmouseout="simvs_stop()" onmouseover="simvs_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/depth-estimation-tip-2021.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function simvs_start() {
            document.getElementById('simvs_image').style.opacity = "1";
          }

          function simvs_stop() {
            document.getElementById('simvs_image').style.opacity = "0";
          }
          simvs_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/9416235">
          <span class="papertitle">MLDA-Net: Multi-level dual attention-based network for self-supervised monocular depth estimation</span>
        </a>
        <br> 
        <strong>Xibin Song</strong>,
	<a>Wei Li</a>,
        <a>Dingfu Zhou</a>,
        <a>Yuchao Dai</a>,
        <a>Jin Fang</a>,
	<a>Hongdong Li</a>,
	<a>Liangjun Zhang</a>
	<p></p>
        <a href="https://ieeexplore.ieee.org/abstract/document/9416235"> pdf - IEEE Transactions on Image Processing (2021)</a>
        <p></p>
	<p>
	We propose a novel framework, named MLDA-Net, to obtain per-pixel depth maps with shaper boundaries and richer depth details.	 
	</p>
      </td>
    </tr>

   <tr onmouseout="simvs_stop()" onmouseover="simvs_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/depth-estimation-iccv-2021.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function simvs_start() {
            document.getElementById('simvs_image').style.opacity = "1";
          }

          function simvs_stop() {
            document.getElementById('simvs_image').style.opacity = "0";
          }
          simvs_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Self-Supervised_Monocular_Depth_Estimation_for_All_Day_Images_Using_Domain_ICCV_2021_paper.pdf">
          <span class="papertitle">Joint 3d instance segmentation and object detection for autonomous driving</span>
        </a>
        <br>
	<a>Lina Liu</a>,
        <strong>Xibin Song</strong>,
        <a>Mengmeng Wang</a>,
        <a>Yong Liu</a>,
	<a>Liangjun Zhang</a>
	<p></p>
	<br>
        <em>Corresponding author</em>
        <br>
        <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Self-Supervised_Monocular_Depth_Estimation_for_All_Day_Images_Using_Domain_ICCV_2021_paper.pdf"> pdf - ICCV (2021)</a>
        <p></p>
	<p>
	We propose a domain-separated network for self-supervised depth estimation of all-day images. 
	</p>
      </td>
    </tr>

   <tr onmouseout="simvs_stop()" onmouseover="simvs_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/3d-object-detection-cvpr-2020.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function simvs_start() {
            document.getElementById('simvs_image').style.opacity = "1";
          }

          function simvs_stop() {
            document.getElementById('simvs_image').style.opacity = "0";
          }
          simvs_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_Joint_3D_Instance_Segmentation_and_Object_Detection_for_Autonomous_Driving_CVPR_2020_paper.pdf">
          <span class="papertitle">Joint 3d instance segmentation and object detection for autonomous driving</span>
        </a>
        <br>
	<a>Dingfu Zhou</a>,
	<a>Jin Fang</a>,
        <strong>Xibin Song</strong>,
        <a>Liu Liu</a>,
        <a>Junbo Yin</a>,
	<a>Yuchao Dai</a>,
	<a>Hongdong Li</a>,
	<a>Ruigang Yang</a>
	<p></p>
	<br>
        <em>Corresponding author</em>
        <br>
        <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_Joint_3D_Instance_Segmentation_and_Object_Detection_for_Autonomous_Driving_CVPR_2020_paper.pdf"> pdf - CVPR (2020)</a>
        <p></p>
	<p>
	We propose a simple but practical detection framework to jointly predict the 3D BBox and instance segmentation.
	</p>
      </td>
    </tr>


   <tr onmouseout="simvs_stop()" onmouseover="simvs_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/depth-sr-cvpr-2020.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function simvs_start() {
            document.getElementById('simvs_image').style.opacity = "1";
          }

          function simvs_stop() {
            document.getElementById('simvs_image').style.opacity = "0";
          }
          simvs_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Song_Channel_Attention_Based_Iterative_Residual_Learning_for_Depth_Map_Super-Resolution_CVPR_2020_paper.pdf">
          <span class="papertitle">Channel attention based iterative residual learning for depth map super-resolution</span>
        </a>
        <br>
	<strong>Xibin Song</strong>,
	<a>Yuchao Dai</a>,
	<a>Dingfu Zhou</a>,
        <a>Liu Liu</a>,
        <a>Junbo Yin</a>,
	<a>Wei Li</a>,
	<a>Hongdong Li</a>,
	<a>Ruigang Yang</a>
	<p></p>
        <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Song_Channel_Attention_Based_Iterative_Residual_Learning_for_Depth_Map_Super-Resolution_CVPR_2020_paper.pdf"> pdf - CVPR (2020)</a>
        <p></p>
	<p>
	We propose a new framework for real-world DSR, which consists of four modules : 1) An iterative residual learning module with deep supervision to learn effective high-frequency components of depth maps in a coarse-to-fine manner; 2) A channel attention strategy to enhance channels with abundant high-frequency components; 3) A multi-stage fusion module to effectively reexploit the results in the coarse-to-fine process; and 4) A depth refinement module to improve the depth map by TGV regularization and input loss. 
	</p>
      </td>
    </tr>

   <tr onmouseout="simvs_stop()" onmouseover="simvs_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/apollo.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function simvs_start() {
            document.getElementById('simvs_image').style.opacity = "1";
          }

          function simvs_stop() {
            document.getElementById('simvs_image').style.opacity = "0";
          }
          simvs_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Song_ApolloCar3D_A_Large_3D_Car_Instance_Understanding_Benchmark_for_Autonomous_CVPR_2019_paper.pdf">
          <span class="papertitle">Apollocar3d: A large 3d car instance understanding benchmark for autonomous driving</span>
        </a>
        <br> 
	<strong>Xibin Song</strong>,
	<a>Peng Wang</a>,
	<a>Dingfu Zhou</a>,
        <a>Rui Zhu</a>,
	<a>Chenye Guan</a>,
        <a>Yuchao Dai</a>,
	<a>Hao Su</a>,
	<a>Hongdong Li</a>,
	<a>Ruigang Yang</a>
	<p></p>
        <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Song_ApolloCar3D_A_Large_3D_Car_Instance_Understanding_Benchmark_for_Autonomous_CVPR_2019_paper.pdf"> pdf - CVPR (2019)</a>
        <p></p>
	<p>
	In this paper, we contribute the first largescale database suitable for 3D car instance understanding â€“ ApolloCar3D.
	</p>
      </td>
    </tr>

   <tr onmouseout="simvs_stop()" onmouseover="simvs_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/depthsr-tcsvt.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function simvs_start() {
            document.getElementById('simvs_image').style.opacity = "1";
          }

          function simvs_stop() {
            document.getElementById('simvs_image').style.opacity = "0";
          }
          simvs_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/1808.08688">
          <span class="papertitle">Deeply supervised depth map super-resolution as novel view synthesis</span>
        </a>
        <br> Xibin Song, Yuchao Dai, Xueying Qin
	<strong>Xibin Song</strong>,
        <a>Yuchao Dai</a>,
	<a>Xueying Qin</a>
	<p></p>
        <a href="https://arxiv.org/pdf/1808.08688"> pdf - IEEE Transactions on circuits and systems for video technology (2018)</a>
        <p></p>
	<p>
		First, we propose to represent the task of depth map super-resolution as a series of novel view synthesis sub-tasks. The novel view synthesis sub-task aims at generating (synthesizing) a depth map from different camera pose, which could be learned in parallel. Second, to handle large up-sampling factors, we present a deeply supervised network structure to enforce strong supervision in each stage of the network. Third, a multi-scale fusion strategy is proposed to effectively exploit the feature maps at different scales and handle the blocking effect.
	</p>
      </td>
    </tr>

  </body>
</html>
